$$$想要详细了解数据集制作$$没看懂怎么构建数据集的 或 想要详细了解如何构建数据集的 请来这里 https://github.com/KMnO4-zx/huanhuan-chat 或者 https://github.com/datawhalechina/self-llm/tree/master/examples
$$$效果不好或者没有效果怎么办？$$尝试用 Qwen_v2.5_instruct 模型重新训练试试，更多问题可以把截图 + 任务ID发给助教。
$$$为什么要排这么久的队？$$实际上微调是很消耗算力的，大的模型可能一次微调就需要占据4张GPU的卡，微调时间跟模型大小、迭代次数、数据集大小都有关系，我们的 AI嬛嬛 至少需要微调10分钟（排上队的话），讯飞平台的GPU资源毕竟有限，我们人太多啦！所以大家可以尽快提交微调任务，然后随便去做一些什么，微调是工作在讯飞的服务器后台的，我们关机都没关系，等他好了我们就可以体验啦！
$$$怎么多轮对话？$$目前只有spark系列模型支持多轮对话
$$$发布失败怎么办？$$平台的问题，应该是卡了或者没有刷新出来，多发布几次，如果过的太久还没有发布成功的话，把任务ID发给助教。也可以更新模型，然后把目前训练完的把呢不能给重新勾选一下试试
$$$平台有哪些功能能帮我分析问题？$$通过【查看】，可以看到任务模型文件，以及Loss曲线，通过Loss曲线可以分析模型过拟合或者欠拟合，来适当调整数据集（e.g.增加多样性）或采取其他策略
$$$找不到“体验”的按钮？$$可能是模型服务没有发布成功，点击左侧服务管控可以查看发布进度。如果长时间没有发布成功就再试一次重新发布
$$$生成的内容没有照搬数据集里的内容$$微调过程是让模型学习特定领域或任务的模式和结构，而不是直接记住数据集中的具体内容。微调后的模型基于先前训练的大规模语言模型基础上，针对特定的数据集进行参数调整，从而生成符合特定领域需求的输出。LLM生成内容的方式是基于概率的。每个生成的词语或句子是通过模型预测下一个最可能的词或短语，而不是从数据集中直接检索。模型通过对数据中的语言结构和信息进行抽象学习，而非直接记忆。